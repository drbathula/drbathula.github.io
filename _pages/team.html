---
layout: archive
title: "CBIL Team"
permalink: /team/
author_profile: true
---

{% include base_path %}

<script type="text/javascript" src="../assets/js/hidebib.js"></script>

<h1> Doctoral Students </h1>
<table width="100%" align="center" border="0" border-spacing="0" cellspacing="0" cellpadding="15" style="border:none;">
<br/>
  
<tr style="border:none;">
  <td width="30%" style="vertical-align: top; border:none;" valign="top" align="center">
  <a href="https://diffusion-classifier.github.io/"></a>
    <img src="../images/Abhishek.jpg" alt="sym" width="90%" style="padding:10px;border-radius:15px;border:1px solid black">
  </a></td>
  <td width="70%" style="vertical-align: top; border:none;" valign="top">
    <h2 style="margin-top: 0px;">Abhishek Singh Sambyal </h2>
    <p>
    <!--<a href="https://diffusion-classifier.github.io/" id="MEMMPD"></a>-->
    His research interests lie in deep learning and medical image analysis. Specifically, he is working on problems related to the uncertainty quantification/calibration in deep neural networks for medical imaging applications. Prior to IIT Ropar, he completed his M. Tech. from Bangalore Institute of Technology. He spent two years at Kudos Knowledge as a Software Engineer in Bangalore and as an Assistant Professor at Central University of Jammu.
    </p>

    <div class="paper" id="MEMMPD">
    <a href="https://abhisheksambyal.github.io/">Webpage</a> |
    <a href="linkedin.com/in/abhishek-singh-sambyal">LinkedIn</a> |  
    <a href="https://abhisheksambyal.github.io/">Webpage</a> |
    <a href="https://scholar.google.co.in/citations?user=FX5YpV8AAAAJ&hl=en">GoogleScholar</a> |-->
    </div>
  </td>
</tr>

<tr style="border:none;">
  <td width="30%" style="vertical-align: top; border:none;" valign="top" align="center">
   <!--<a href="https://diffusion-classifier.github.io/"></a>-->
  <img src="../images/Ranjana.jpg" alt="sym" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;border:1px solid black">
  </td>
  <td width="70%" style="vertical-align: top; border:none;" valign="top">
    <h2 style="margin-top: 0px;">Ranjana Roy Chowdhury</h2>
    <p>
    <!--<a href="https://diffusion-classifier.github.io/" id="WAVEKD"></a>-->
    Ranjana Roy Chowdhury earned her B.Tech degree in CSE from Assam University Silchar and M.Tech degree in Services Computing from CSE Department, IIIT Guwahati. Her research areas focus on Few Shot Learning for Medical Image Analysis. Currently, she is working towards exploring various applications of Meta Learning Approaches towards solving few shot learning problems in the medical imaging domain.
    </p>

    <div class="paper" id="WAVEKD">
    <!--<a href="https://diffusion-classifier.github.io/">webpage</a> |-->
    <a href="javascript:toggleblock('wavekd_abs')">abstract</a> |
    <a shape="rect" href="javascript:togglebib('WAVEKD')" class="togglebib">bibtex</a> |
    <a href="https://arxiv.org/abs/2303.16203">arXiv</a> 
    <!--<a href="https://github.com/diffusion-classifier/diffusion-classifier">code</a>-->

    <p class="abstract_text" align="justify"> <span id="wavekd_abs" style="display: none;">Deep learning (DL) models can achieve state-of-the-art performance but at the cost of high computation and memory requirements. Due to their large capacity, DL models have the tendency to learn redundant features. In this work, we exploit this redundancy to improve model compression. Knowledge distillation (KD) aims to achieve model compression by transferring knowledge from a large, complex model to a simple, lightweight model. We propose an enhanced KD strategy that improves the efficiency of the distillation process by compressing the feature maps using Discrete Wavelet Transformation DWT, which helps capture crucial features from complex biomedical signals. Retaining and sharing only the most informative and discriminating features facilitates more effective feature mimicking. Extensive experiments using two benchmark datasets for Melanoma and Histopathology image classification tasks demonstrate the superiority of our proposed method over existing techniques. We further establish the generalizability and robustness of our method using two different teacher-student architectures and ablation studies.</span></p>

<pre xml:space="preserve" style="display:none;">
@inproceedings{Niyaz2024ISBI,
  title={Wavelet-Based Feature Compression For Improved Knowledge Distillation},
  author={Niyaz, Usma and Sambyal, Abhishek Singh and Bathula, Deepti R.},
  booktitle={ISBI},
  year={2024}}
</pre>
    </div>
  </td>
</tr>
  
</table>

<h1> Alumni </h1>

